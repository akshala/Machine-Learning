# -*- coding: utf-8 -*-
"""ML_Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CipmvCDkyaB33E6vRjNhpUFOVXpRFce4
"""

# from google.colab import drive
# drive.mount('/content/drive')

# cd /content/drive/MyDrive/ML_HW3

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as data

import torchvision.transforms as transforms
import torchvision.datasets as datasets

from sklearn import metrics
from sklearn import decomposition
from sklearn import manifold
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

import copy
import random
import time

"""Setting seed for random values"""

SEED = 1234
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

class Dataset(torch.utils.data.Dataset):

  def __init__(self, fileName):
    df = pd.read_csv(fileName, header = None) 
    self.y = np.array(df[0])                # first column is labels
    data = np.array(df) 
    self.x = np.delete(data, 0, axis=1)     # removing first column which corresponds to labels
    self.x = self.x.astype(np.float32)     

  def __len__(self):
    return len(self.y)

  def __getitem__(self, index):
    return self.x[index], self.y[index]

class MLP(nn.Module):
    def __init__(self, input_dim, output_dim, n_hidden_units):
      super().__init__()
              
      self.input_fc = nn.Linear(input_dim, n_hidden_units)
      torch.nn.init.uniform(self.input_fc.weight)
      self.output_fc = nn.Linear(n_hidden_units, output_dim)
      torch.nn.init.uniform(self.output_fc.weight)
        
    def forward(self, x):
      batch_size = x.shape[0]
      x = x.view(batch_size, -1)

      hidden = F.relu(self.input_fc(x))
      y_pred = self.output_fc(hidden)
      return y_pred, hidden

def train(model, iterator, optimizer, criterion, device):
   """
        Training model

        Parameters
        ----------
        model : neural network model to be used for training
        optimizer : optimizer used
        criterion : loss function used
        iterator : Dataset type object which contains train data
        device : for using GPU

        Returns
        -------
        loss
        """
    epoch_loss = 0
    model.train()
    
    for (x, y) in iterator:
        x = x.to(device)
        y = y.to(device)
        
        optimizer.zero_grad()       
        y_pred, _ = model(x)
        loss = criterion(y_pred, y)
        loss.backward()  
        optimizer.step()
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion, device): 
  """
        Evaluating model

        Parameters
        ----------
        model : neural network model to be used for training
        iterator : Dataset type object which contains train data
        device : for using GPU

        Returns
        -------
        loss
        """ 
    epoch_loss = 0 
    model.eval()
    
    with torch.no_grad():
        for (x, y) in iterator:
            x = x.to(device)
            y = y.to(device)

            y_pred, _ = model(x)
            loss = criterion(y_pred, y)
            epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)

def neural_network(n_hidden_unit, learning_rate, train_iterator, val_iterator):
  """
        Evaluating model

        Parameters
        ----------
        n_hidden_unit : number of hidden units the model is to be run for
        learning_rate : learning rate for model
        train_iterator : Dataset type object which contains train data
        val_iterator : Dataset type object which contains test data

        Returns
        -------
        loss values
        """ 
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    # for GPU use
  model = MLP(128, 10, n_hidden_unit)
  optimizer = optim.Adam(model.parameters(), lr = learning_rate)
  criterion = nn.CrossEntropyLoss()
  model.to(device)                       # for GPU use
  criterion = criterion.to(device)        # for GPU use

  all_train_loss = []
  all_val_loss = []

  for epoch in range(100):  
    train_loss = train(model, train_iterator, optimizer, criterion, device)
    all_train_loss.append(train_loss)
    val_loss = evaluate(model, val_iterator, criterion, device)
    all_val_loss.append(val_loss)

  return (train_loss, val_loss, all_train_loss, all_val_loss)

train_iterator = data.DataLoader(Dataset('largeTrain.csv'), shuffle = True, batch_size = 90)
val_iterator = data.DataLoader(Dataset('largeValidation.csv'), batch_size = 90)


all_train_loss = []
all_val_loss = []
n_hidden_units = [5, 20, 50, 100, 200]
for n in n_hidden_units:
  print('num layers', n)
  loss = neural_network(n, 0.01, train_iterator, val_iterator)
  all_train_loss.append(loss[0])
  all_val_loss.append(loss[1])

plt.plot(n_hidden_units, all_train_loss, label='train') 
plt.plot(n_hidden_units, all_val_loss, label='val') 
plt.xlabel('Number of hidden units') 
plt.ylabel('Average cross entropy') 
plt.title('Hidden units and cross entropy loss') 
plt.legend()
plt.savefig('Q3_hidden.png')

learning_rate = [0.1, 0.01, 0.001]
for l in learning_rate:
	loss = neural_network(4, l, train_iterator, val_iterator)
	all_train_loss = loss[2]
	all_val_loss = loss[3]
	epochs = [i for i in range(100)]

	plt.plot(epochs, all_train_loss, label='train') 
	plt.plot(epochs, all_val_loss, label='val') 
	plt.xlabel('Numnber of epochs') 
	plt.ylabel('Average cross entropy') 
	plt.title('Learning rate: ' + str(l)) 
	plt.legend()
	plt.savefig('Q3_lr_' + str(l) + '.png')
	plt.clf()

